{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Learning Model - Production\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda update scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from sklearn.feature_extraction import TfidfVectorizer ===> gets an error\n",
    "# pls use the import below\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "from scipy.sparse import hstack, vstack                            # used on bow of workds, vetoctored the 'Title' column\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load final raw data with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape before:  (727, 5)\n",
      "Dataset columns:  Index(['title', 'y', 'upload_date', 'view_count', 'new'], dtype='object')\n",
      "Dataset shape after:  (701, 5)\n",
      "Check duplicates:  0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>y</th>\n",
       "      <th>upload_date</th>\n",
       "      <th>view_count</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>Data Scientists vs Data Engineers: Which one i...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20191219</td>\n",
       "      <td>183723</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>#kaggle #DataScience Machine Learning MicroCou...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20190730</td>\n",
       "      <td>405</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title    y  upload_date  \\\n",
       "1016  Data Scientists vs Data Engineers: Which one i...  1.0     20191219   \n",
       "1468  #kaggle #DataScience Machine Learning MicroCou...  0.0     20190730   \n",
       "\n",
       "      view_count  new  \n",
       "1016      183723  1.0  \n",
       "1468         405  1.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./raw_data/final_labels_dataset.csv', index_col=0)\n",
    "\n",
    "print('Dataset shape before: ', df.shape)\n",
    "print('Dataset columns: ', df.columns)\n",
    "\n",
    "# drop duplicated data\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "print('Dataset shape after: ', df.shape)\n",
    "\n",
    "print('Check duplicates: ', df.duplicated().mean())\n",
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Cleanup\n",
    "\n",
    "Using a NEW dataframe with data that's ready and clean the data to fit the ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a clean dataframe with the same indice on the original dataframe - raw data\n",
    "df_clean = pd.DataFrame(index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['date'] = pd.to_datetime(df['upload_date'], format='%Y%m%d')\n",
    "\n",
    "# note: format='%Y %m %d' shows the time; format='%Y%m%d' brings only YYYY-MM-DD - easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2021-05-05\n",
       "1      2021-05-05\n",
       "2      2021-05-05\n",
       "3      2021-05-05\n",
       "4      2021-05-05\n",
       "          ...    \n",
       "1041   2019-11-18\n",
       "488    2018-11-25\n",
       "528    2018-07-24\n",
       "1016   2019-12-19\n",
       "1468   2019-07-30\n",
       "Name: date, Length: 701, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean['date']\n",
    "\n",
    "# dtype: datetime64[ns] used by numpy and pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns views: make sure all NAN will be convert to 0 and an integer data type will be added\n",
    "df_clean['views'] = df['view_count'].fillna(0).astype(int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding title column. It will be used on the model....will be vectorized later\n",
    "df_clean['title'] = df['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date     datetime64[ns]\n",
       "views             int64\n",
       "title            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Features & Labels\n",
    "\n",
    "Create an unique features dataframe. JUST an extra step. Making sure the features are ready.\n",
    "\n",
    "**Reason**: Align the feaatures dataframe with the most cleaning data - raw data collected & cleaned. The cleaning process can skip rows or columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features: it's similar to df_clean, just an extra step\n",
    "features = pd.DataFrame(index=df_clean.index)\n",
    "\n",
    "# labels/targets\n",
    "y = df['y'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (701, 0)\n",
      "Labels shape: (701,)\n"
     ]
    }
   ],
   "source": [
    "print('Features shape: {}'.format(shape(features)))\n",
    "print('Labels shape: {}'.format(shape(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important: sklearn can't use *date* as a feature.\n",
    "\n",
    "Let's manipulate and create a feature using the raw date - **Num_views_per_day**.\n",
    "\n",
    "Sklearn needs a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_since_pub: time since the video was published. Random data choose. Use the date I created this code: fix date point - 2021-05-09\n",
    "\n",
    "# np.timedelta64(1, 'D'): time delta in numpy. Difference in days\n",
    "# we have data on a granually day, meaning a difference less than a day makes sense.\n",
    "features['time_since_pub'] = (pd.to_datetime(\"2021-05-09\") - df_clean['date']) / np.timedelta64(1, 'D')\n",
    "\n",
    "# used features\n",
    "features['views'] = df_clean['views']\n",
    "features['views_per_day'] = features['views'] / features['time_since_pub']\n",
    "\n",
    "features = features.drop(['time_since_pub'], axis=1)   # time_since_pub only used for the calculation\n",
    "\n",
    "# time_since_pub as a feature may impact the model once the numbers seem to increase a lot and the end of the time serie.\n",
    "# The training&validations datasets may not have a normal distributed values.Thus, an umbalaced feature weights\n",
    "# and random samples are important to train and fit a ml model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>views</th>\n",
       "      <th>views_per_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>31788</td>\n",
       "      <td>59.085502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>65724</td>\n",
       "      <td>73.352679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>34286</td>\n",
       "      <td>33.613725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>183723</td>\n",
       "      <td>362.372781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>405</td>\n",
       "      <td>0.624037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       views  views_per_day\n",
       "1041   31788      59.085502\n",
       "488    65724      73.352679\n",
       "528    34286      33.613725\n",
       "1016  183723     362.372781\n",
       "1468     405       0.624037"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>views</th>\n",
       "      <th>views_per_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.010000e+02</td>\n",
       "      <td>701.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.525490e+05</td>\n",
       "      <td>521.359455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.042103e+06</td>\n",
       "      <td>4284.958273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.390000e+02</td>\n",
       "      <td>4.977941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.892000e+03</td>\n",
       "      <td>37.956522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.997200e+04</td>\n",
       "      <td>180.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.872417e+07</td>\n",
       "      <td>95913.728346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              views  views_per_day\n",
       "count  7.010000e+02     701.000000\n",
       "mean   1.525490e+05     521.359455\n",
       "std    2.042103e+06    4284.958273\n",
       "min    0.000000e+00       0.000000\n",
       "25%    3.390000e+02       4.977941\n",
       "50%    2.892000e+03      37.956522\n",
       "75%    1.997200e+04     180.000000\n",
       "max    4.872417e+07   95913.728346"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "Let's try to split the train&validation datasets 50/50.\n",
    "\n",
    "How the 2 features **view** and **views_per_day** impacted the ML model? \n",
    "Does a simple model with only 2 features impact the way the YouTube videos will be selected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2021-01-10 00:00:00')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check all data on df_clean\n",
    "# pd.set_option('display.max_rows', 527)\n",
    "# df_clean\n",
    "\n",
    "median_date = df_clean['date'].quantile(0.5, interpolation=\"midpoint\")\n",
    "median_date\n",
    "\n",
    "# median date 2021-03-12 before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Increasing the validation set\n",
    "\n",
    "**Note**: prior models, without active learning samples, we split the dataset train and validation with only the **median date**.\n",
    "\n",
    "## Active Learning samples - usually are added into the training dataset\n",
    "\n",
    "## However, we'll try to add them into the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((349, 2), (352, 2), (349,), (352,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting features dataset - trying a 50/50 using a median date\n",
    "# balanced dataset is important!!!\n",
    "\n",
    "# code below can also be used\n",
    "# Xtrain, Xval = features[df_clean['date'] < '2021-03-12'], features[df_clean['date'] >= '2021-03-12']\n",
    "# ytrain, yval = y[df_clean['date'] < '2021-03-12 '], y[df_clean['date'] >= '2021-03-12 ']\n",
    "\n",
    "# needed approach - mask parameter to select the data\n",
    "mask_train = df_clean['date'] < '2021-01-10'\n",
    "mask_val = df_clean['date'] >= '2021-01-10'\n",
    "\n",
    "Xtrain, Xval = features[mask_train], features[mask_val]\n",
    "ytrain, yval = y[mask_train], y[mask_val]\n",
    "\n",
    "Xtrain.shape, Xval.shape, ytrain.shape, yval.shape\n",
    "\n",
    "# datasets, training & validation, not huge. But,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the title feature\n",
    "\n",
    "**Important**: transforming the Title string to numbers.\n",
    "\n",
    "Building a matrix in which each column will be the counting word from the Title feature.\n",
    "\n",
    "Import to notice that commom words like machine+learning will have a low weight.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_train = df_clean[mask_train]['title']\n",
    "title_val = df_clean[mask_val]['title']\n",
    "\n",
    "# Vectorizing the Title features\n",
    "title_vec = TfidfVectorizer(min_df=2, ngram_range=(1,3))   # object defined\n",
    "# mind_df = 2 means the minimum numnber of words that be used to create a column\n",
    "# ngram_range=(1,3) - combining words to maximum 3 words\n",
    "\n",
    "# bow: bag of words\n",
    "title_bow_train = title_vec.fit_transform(title_train)     # fit + transform: store the words on the features, plus how many times the word appeared\n",
    "title_bow_val = title_vec.transform(title_val)             # validation set ONLY transform. Validation should NOT learning the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without ngram_range=(1,3): Shape for title bag of words matrix:  (349, 289)\n",
    "with ngram_range=(1,3):   Shape for title bag of words matrix:  (349, 719)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape for title bag of words matrix:  (349, 719)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<349x719 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3968 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking\n",
    "print('Shape for title bag of words matrix: ', title_bow_train.shape)\n",
    "title_bow_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " TfidfVectorizer function returns a vectorized sparse matrix. It's an optimize matrix in Scipy where only values NOT equal to zero are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9908899259158892"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the sparse matrix 'title_bow_train' contains 3968  elements NOT ZERO\n",
    "1 - 2286 /(349*719)   \n",
    "# % of ZERO elements on the sparse matrix, but only 3% are NOT ZERO elements. Meaning that the matrix is sparse computationally and mathematically speaking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT to note: \n",
    "Combining simple matrix - Xtrain&Xval - with a sparse matrix - title_bow_train & title_bow_val\n",
    "\n",
    "Use scipy.sparse hstack and vstack\n",
    "\n",
    "More details on hstack and vstack...stacking matrix (vectoes) horizontally and vertically\n",
    "\n",
    "Sample:\n",
    "\n",
    "hstack - [1 2]    [3 4]  -> [1 2 3 4]\n",
    "\n",
    "vstack [1 2]      [3 4]  -> [1 2]\n",
    "                            [3 4]\n",
    "                            \n",
    "USE *scipy.sparse hstack and vstack*, numpy sparse function may take TOO LONG, or not compute at all!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining sparse matrix with original features\n",
    "from scipy.sparse import hstack, vstack  \n",
    "\n",
    "Xtrain_wtitle = hstack([Xtrain, title_bow_train])\n",
    "Xval_wtitle = hstack([Xval, title_bow_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((349, 721), (352, 721))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_wtitle.shape, Xval_wtitle.shape\n",
    "\n",
    "# 2 nummerical features on training dataset plus 289 columns from 'Title'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples - videos select: 107.0\n",
      " % of positive samples - videos select: 30.659025787965614\n"
     ]
    }
   ],
   "source": [
    "# check number of 1 samples under train dataset\n",
    "print('Positive samples - videos select: {}'.format(ytrain.mean() * 349))\n",
    "print(' % of positive samples - videos select: {}'.format(ytrain.mean() * 100))\n",
    "\n",
    "# definitely unbalaced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset is not big!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf = RandomForestClassifier(n_estimators=1000, random_state=0, min_samples_leaf=1, class_weight='balanced', n_jobs=6)    # defined object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model against the train dataset\n",
    "\n",
    "NOW: 3 features - views, views per day, and title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
           ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       n_estimators=1000, n_jobs=6, oob_score=False,\n",
       "                       random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf.fit(Xtrain_wtitle, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML model already trainded/fitted and ready to be used!\n"
     ]
    }
   ],
   "source": [
    "print('ML model already trainded/fitted and ready to be used!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting if a video has been select\n",
    "\n",
    "Probability = 1\n",
    "\n",
    "predict_proba: returns a numpy array with prob of zero and prob of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      
     ]
    }
   ],
   "source": [
    "pred = clf_rf.predict_proba(Xval_wtitle)[:, 1]   # only 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics - validating the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Before: Random Forest Model\n",
    "\n",
    "average precision: 0.4566659359075169\n",
    "\n",
    "Roc Auc: 0.5182456140350877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest baseline model - average precision\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4605227155328958"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# area of precision for decision tree\n",
    "print('Random Forest baseline model - average precision')\n",
    "average_precision_score(yval, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT: any future model in PRD should have a greater than baseline model **0.50**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest baseline model - roc auc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5253960396039604"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# area under curve of roc curve metric\n",
    "print('Random Forest baseline model - roc auc')\n",
    "roc_auc_score(yval, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating metrics before and after the active learning samples\n",
    "\n",
    "Increassing the validation dataset metrics:\n",
    "\n",
    "roc auc: 0.47\n",
    "\n",
    "average precision: 0.41\n",
    "\n",
    "----------------------------------------------------------\n",
    "Original decision metric - baseline model\n",
    "\n",
    "roc auc: 0.49\n",
    "\n",
    "average precision: 0.42\n",
    "\n",
    "---------------------------------------------------------\n",
    "\n",
    "Metrics seem to be on the variation range. Meaning AP & ROC AUC are close enough between baseline model and new model with active learning samples.\n",
    "Please note that the number of samples are not significantly big, then the metrics might not change a lot...small steps definitely count!!!\n",
    "\n",
    "---------------------------------------------------------\n",
    "\n",
    "## Productin Baseline Model\n",
    "roc auc: 0.46\n",
    "\n",
    "average precision: 0.52\n",
    "\n",
    "**Details**: TfidfVectorizer(min_df=2, ngram_range=(1,3)) \n",
    "---------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LigthGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_lgbm = LGBMClassifier(random_state=0, class_weight='balanced', n_jobs=6)   # defined object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight='balanced',\n",
       "               colsample_bytree=1.0, importance_type='split', learning_rate=0.1,\n",
       "               max_depth=-1, min_child_samples=20, min_child_weight=0.001,\n",
       "               min_split_gain=0.0, n_estimators=100, n_jobs=6, num_leaves=31,\n",
       "               objective=None, random_state=0, reg_alpha=0.0, reg_lambda=0.0,\n",
       "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
       "               subsample_freq=0)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl_lgbm.fit(Xtrain_wtitle, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      
     ]
    }
   ],
   "source": [
    "pred = cl_lgbm.predict_proba(Xval_wtitle)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest baseline model - average precision:  0.4052786105139529\n",
      "Random Forest baseline model - roc auc:  0.48495049504950494\n"
     ]
    }
   ],
   "source": [
    "# area of precision for decision tree\n",
    "print('Random Forest baseline model - average precision: ', average_precision_score(yval, pred))\n",
    "\n",
    "# area under curve of roc curve metric\n",
    "print('Random Forest baseline model - roc auc: ', roc_auc_score(yval, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optmization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_train\n",
    "title_val\n",
    "\n",
    "title_vec = TfidfVectorizer(min_df=2)\n",
    "title_bow_train = title_vec.fit_transform(title_train)\n",
    "title_bow_val = title_vec.transform(title_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed approach - mask parameter to select the data\n",
    "mask_train = df_clean['date'] < '2021-01-10'\n",
    "mask_val = df_clean['date'] >= '2021-01-10'\n",
    "\n",
    "Xtrain, Xval = features[mask_train], features[mask_val]\n",
    "ytrain, yval = y[mask_train], y[mask_val]\n",
    "\n",
    "title_train = df_clean[mask_train]['title']\n",
    "title_val = df_clean[mask_val]['title']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import forest_minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "[0.009944912110647982, 5, 1, 0.4677107511929402, 0.49263223036174764, 272, 3, 1]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-98750125954e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m           (1,5)] # ngram_range\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforest_minimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtune_lgbm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m160745\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_random_starts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_calls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\forest.py\u001b[0m in \u001b[0;36mforest_minimize\u001b[1;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, acq_func, initial_point_generator, x0, y0, random_state, verbose, callback, n_points, xi, kappa, n_jobs, model_queue_size)\u001b[0m\n\u001b[0;32m    194\u001b[0m                          \u001b[0mxi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkappa\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkappa\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                          \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macq_optimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sampling\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                          model_queue_size=model_queue_size)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[1;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs, model_queue_size)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_calls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[0mnext_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m         \u001b[0mnext_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-111-98750125954e>\u001b[0m in \u001b[0;36mtune_lgbm\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mtitle_bow_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtitle_vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mXtrain_wtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle_bow_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mXval_wtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mXval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle_bow_val\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mhstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mhstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)"
     ]
    }
   ],
   "source": [
    "def tune_lgbm(params):\n",
    "    print(params)\n",
    "    lr = params[0]\n",
    "    max_depth = params[1]\n",
    "    min_child_samples = params[2]\n",
    "    subsample = params[3]\n",
    "    colsample_bytree = params[4]\n",
    "    n_estimators = params[5]\n",
    "    \n",
    "    min_df = params[6]\n",
    "    ngram_range = (1, params[7])\n",
    "    \n",
    "    title_vec = TfidfVectorizer(min_df=min_df, ngram_range=ngram_range)\n",
    "    title_bow_train = title_vec.fit_transform(title_train)\n",
    "    title_bow_val = title_vec.transform(title_val)\n",
    "    \n",
    "    Xtrain_wtitle = hstack([Xtrain, title_bow_train])\n",
    "    Xval_wtitle = hstack([Xval, title_bow_val])\n",
    "    \n",
    "    mdl = LGBMClassifier(learning_rate=lr, num_leaves=2 ** max_depth, max_depth=max_depth, \n",
    "                         min_child_samples=min_child_samples, subsample=subsample,\n",
    "                         colsample_bytree=colsample_bytree, bagging_freq=1,n_estimators=n_estimators, random_state=0, \n",
    "                         class_weight=\"balanced\", n_jobs=6)\n",
    "    mdl.fit(Xtrain_wtitle, ytrain)\n",
    "    \n",
    "    p = mdl.predict_proba(Xval_wtitle)[:, 1]\n",
    "    \n",
    "    print(roc_auc_score(yval, p))\n",
    "    \n",
    "    return -average_precision_score(yval, p)\n",
    "\n",
    "space = [(1e-3, 1e-1, 'log-uniform'), # lr\n",
    "          (1, 10), # max_depth\n",
    "          (1, 20), # min_child_samples\n",
    "          (0.05, 1.), # subsample\n",
    "          (0.05, 1.), # colsample_bytree\n",
    "          (100,1000), # n_estimators\n",
    "          (1,5), # min_df\n",
    "          (1,5)] # ngram_range\n",
    "\n",
    "res = forest_minimize(tune_lgbm, space, random_state=160745, n_random_starts=20, n_calls=50, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overrall\n",
    "## Model 4: Random Forest with active learning samples\n",
    "## Changing validation and training dataset\n",
    "\n",
    "\n",
    "Active learning is impacting a bit the model, but not enough. Metrics are on the range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Number\tModel\tNumber of Features\tFeatures\tMetric\t#\tRemark\n",
    "1\tBaseline Decision Tree \t2\tviews; views per day\tAverage Precision\t0.42\t\n",
    "1\tBaseline Decision Tree \t2\tviews; views per day\tROC AUC\t0.49\t\n",
    "2\tRandom Forest \t2\tviews; views per day\tAverage Precision\t0.45\t\n",
    "2\tRandom Forest \t2\tviews; views per day\tROC AUC\t0.51\t\n",
    "3\tRandom Forest \t3\tviews; views per day; title\tAverage Precision\t0.41\tincrease validation dataset with active learning samples\n",
    "3\tRandom Forest \t3\tviews; views per day; title\tROC AUC\t0.47\tincrease validation dataset with active learning samples\n",
    "3\tRandom Forest \t3\tviews; views per day; title\tAverage Precision\t0.42\tincrease training dataset with active learning samples\n",
    "3\tRandom Forest \t3\tviews; views per day; title\tROC AUC\t0.5\tincrease training dataset with active learning samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
